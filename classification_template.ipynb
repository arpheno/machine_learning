{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load my_imports.ipy\n",
    "# Stdlib\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "from pprint import pprint as print\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns',102)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.width',120)\n",
    "import sklearn\n",
    "print(f'numpy {np.__version__} pandas {pd.__version__} sklearn {sklearn.__version__}')\n",
    "\n",
    "# Visualization\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "custom_style = {'axes.labelcolor': 'white', 'xtick.color': 'white', 'ytick.color': 'white', }\n",
    "sns.set_style( rc=custom_style)\n",
    "mpl.rcParams['figure.figsize']=(10,10)\n",
    "plt.rcParams['figure.figsize']=(10,10)\n",
    "jtplot.style('monokai')\n",
    "# Custom stuff\n",
    "from swozny_ml import *\n",
    "from genetic.param_opt import tune_params_genetic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load classify.py\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "\n",
    "from genetic.estimator_params import params\n",
    "from genetic.param_opt import tune_params_genetic\n",
    "from swozny_ml import benchmark_models, tune_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data.csv'\n",
    "index = 'shot_id'\n",
    "target = 'shot_made_flag'\n",
    "prediction_params = ['Angle', 'Distance']\n",
    "scoring = 'accuracy'\n",
    "voting = 'soft'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data\n",
    "data = pd.read_csv(filename, index_col=index)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cond = data[target].notnull()\n",
    "X_train = data[training_cond][prediction_params]\n",
    "y_train = data[training_cond][target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark different estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [estimator for estimator in params if \"Classifier\" in estimator.__name__] + [LogisticRegression]\n",
    "benchmark = benchmark_models(classifiers, X_train, y_train, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report estimator performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_algorithms = benchmark.sort_values('Mean').tail(5)\n",
    "plot_benchmark(benchmark)\n",
    "print(considered_algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the best estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    tuned = tune_params(considered_algorithms['Algorithm'], X_train, y_train, scoring='accuracy')\n",
    "else:\n",
    "    tuned = tune_params_genetic(considered_algorithms['Algorithm'], X_train, y_train, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrate the tuned estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated = [CalibratedClassifierCV(model).fit(X_train, y_train) for model in tuned]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Correlation between estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat(\n",
    "    [pd.Series(model.predict(X_train), name=type(model.base_estimator).__name__) for model in calibrated], axis=1)\n",
    "sns.heatmap(predictions.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_model = VotingClassifier([(type(model.base_estimator).__name__, model) for model in calibrated], voting=voting)\n",
    "survival_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(survival_model, X=X_train, y=y_train)\n",
    "score = cross_val_score(survival_model, X=X_train, y=y_train)\n",
    "print(f\"Final cross validation score is {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_pred, y_train), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cond = ~training_cond\n",
    "X_test = data[test_cond][prediction_params]\n",
    "y_pred = survival_model.predict_proba(X_test)\n",
    "X_test['shot_made_flag'] = y_pred[:, 1]\n",
    "X_test.shot_made_flag.to_csv('pred_kobe.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
